{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "hide_input": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "name": "a4_rewritten.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b612f1fa45fe49a484445de5e7055d80": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_dea1fc9dcb194fc9b814fc1199d304ce",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_9d3d3af13b9c43e29f774e4ec580cdab",
              "IPY_MODEL_20d00b598f9a4d90b2ea16b05cb237da"
            ]
          }
        },
        "dea1fc9dcb194fc9b814fc1199d304ce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9d3d3af13b9c43e29f774e4ec580cdab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_34fb7f36719d46aea4cdfe4a8bbb2d07",
            "_dom_classes": [],
            "description": "Decoding: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 8064,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 8064,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_522303df1264487a81eed81c9a0c44da"
          }
        },
        "20d00b598f9a4d90b2ea16b05cb237da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_bdf946a77a834d8a810d3ce5e9b059d2",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 8064/8064 [11:49&lt;00:00, 11.37it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e1148982764a4337b8bb914641915aa6"
          }
        },
        "34fb7f36719d46aea4cdfe4a8bbb2d07": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "522303df1264487a81eed81c9a0c44da": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bdf946a77a834d8a810d3ce5e9b059d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e1148982764a4337b8bb914641915aa6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "Qeuvu5bC9yF2",
        "outputId": "7343e899-7d13-4fe9-cc57-9a4eec76126a"
      },
      "source": [
        "!pip install wandb -qqq\n",
        "import wandb\n",
        "wandb.login()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 1.8MB 5.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 81kB 9.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 102kB 9.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 163kB 21.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 133kB 16.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 8.7MB/s \n",
            "\u001b[?25h  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "wandb: Paste an API key from your profile and hit enter: ··········\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HvsGyL4Z_ggZ",
        "outputId": "ed4e47a8-9ff8-4fea-daca-f00584143e97"
      },
      "source": [
        "# цепляю диск\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DMyKRQ-J4FuE",
        "outputId": "c723910c-55eb-4d30-d009-73221808fdaf"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as f\n",
        "import torch.nn.utils\n",
        "from typing import List, Tuple, Dict\n",
        "from collections import namedtuple\n",
        "import nltk\n",
        "\n",
        "import math\n",
        "import numpy as np\n",
        "from nltk.translate.bleu_score import corpus_bleu, sentence_bleu, SmoothingFunction\n",
        "from docopt import docopt\n",
        "import json\n",
        "\n",
        "import sys\n",
        "import time\n",
        "\n",
        "import pickle\n",
        "\n",
        "Hypothesis = namedtuple('Hypothesis', ['value', 'score'])\n",
        "\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tiFEXhEO4FuH"
      },
      "source": [
        "<div class='alert alert-block alert-info'>\n",
        "<b>model_embeddings.py</b>\n",
        "    <ul>\n",
        "        <li><i>Нахуя нам вообще Vocab и VocabEntry?</i> А мы тут ахуенно <b>сторим слова.</b> Нам необходимо связять имбеддинги, которые мы выучим, со словами. А вокабы – маппинг слов и индексов, который мы берем из json'ов\n",
        "        <li><i>Как работает nn.Embeddings?</i> Лукап таблица индекс – имбеддинг.\n",
        "    </ul>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [
          "vocab.py",
          "pt1"
        ],
        "id": "vIf-WmSO4FuH"
      },
      "source": [
        "class VocabEntry(object):\n",
        "    \"\"\"\n",
        "    Vocabulary Entry, i.e. structure containing either src or tgt language terms.\n",
        "    \"\"\"\n",
        "    def __init__(self, word2id: dict=None):\n",
        "        if word2id:\n",
        "            self.word2id = word2id\n",
        "        else:\n",
        "            self.word2id = dict()\n",
        "            self.word2id['<pad>'] = 0  # Pad Token\n",
        "            self.word2id['<s>'] = 1    # Start Token\n",
        "            self.word2id['</s>'] = 2   # End Token\n",
        "            self.word2id['<unk>'] = 3  # Unknown Token\n",
        "        \n",
        "        self.unk_id = self.word2id['<unk>']\n",
        "        self.id2word = {v: k for k, v in self.word2id.items()}\n",
        "        \n",
        "    def __getitem__(self, word: str) -> int:\n",
        "        return self.word2id.get(word, self.unk_id)\n",
        "    \n",
        "    def __contains__(self, word: str) -> bool:\n",
        "        return word in self.word2id\n",
        "    \n",
        "    def __len__(self) -> int:\n",
        "        return len(self.word2id)\n",
        "    \n",
        "    def id2word(self, idx) -> str:\n",
        "        return self.id2word[idx]\n",
        "    \n",
        "    def words2indices(self, sentences):\n",
        "        return [self[word] for word in sentences]\\\n",
        "            if type(sentences[0]) == str\\\n",
        "            else [[self[word] for word in sentence] for sentence in sentences]\n",
        "    \n",
        "    def indices2words(self, indices: List[int]) -> List[str]:\n",
        "        return [self.id2word[index] for index in indices]\n",
        "    \n",
        "    # Convert list of sentences (words) into tensor with necessary padding for shorter sentences\n",
        "    def to_input_tensor(self, sents: List[List[str]], device: torch.device) -> torch.Tensor:\n",
        "        sents = self.words2indices(sents)\n",
        "        \n",
        "        # (batch, sentence_len)\n",
        "        sents_padded = []\n",
        "        max_length = len(max(sents, key=len))\n",
        "        for sent in sents:\n",
        "            sents_padded.append(sent + [self.word2id['<pad>']] * (max_length - len(sent)))\n",
        "        \n",
        "        return torch.tensor(sents_padded, dtype=torch.long, device=device).permute(1, 0)\n",
        "    \n",
        "    # ЛЕНЬ СУКА\n",
        "    def __repr__(self):\n",
        "        return 'Vocabulary[size=%d]' % len(self)\n",
        "    \n",
        "    def __setitem__(self, key, value):\n",
        "        raise ValueError('vocabulary is readonly')\n",
        "        \n",
        "    def add(self, word):\n",
        "        if word not in self:\n",
        "            wid = self.word2id[word] = len(self)\n",
        "            self.id2word[wid] = word\n",
        "            return wid\n",
        "        else:\n",
        "            return self[word]\n",
        "        \n",
        "    @staticmethod\n",
        "    def from_corpus(corpus, size, freq_cutoff=2):\n",
        "        vocab_entry = VocabEntry()\n",
        "        word_freq = Counter(chain(*corpus))\n",
        "        valid_words = [w for w, v in word_freq.items() if v >= freq_cutoff]\n",
        "        print('number of word types: {}, number of word types w/ frequency >= {}: {}'\n",
        "              .format(len(word_freq), freq_cutoff, len(valid_words)))\n",
        "        top_k_words = sorted(valid_words, key=lambda w: word_freq[w], reverse=True)[:size]\n",
        "        for word in top_k_words:\n",
        "            vocab_entry.add(word)\n",
        "        return vocab_entry\n",
        "    \n",
        "    \n",
        "class Vocab(object):\n",
        "    \n",
        "    def __init__(self, src_vocab: VocabEntry, tgt_vocab: VocabEntry):\n",
        "        self.src = src_vocab\n",
        "        self.tgt = tgt_vocab\n",
        "    \n",
        "    # ЛЕНЬ СУКА\n",
        "    @staticmethod\n",
        "    def build(src_sents, tgt_sents, vocab_size, freq_cutoff) -> 'Vocab':\n",
        "        assert len(src_sents) == len(tgt_sents)\n",
        "\n",
        "        print('initialize source vocabulary ..')\n",
        "        src = VocabEntry.from_corpus(src_sents, vocab_size, freq_cutoff)\n",
        "\n",
        "        print('initialize target vocabulary ..')\n",
        "        tgt = VocabEntry.from_corpus(tgt_sents, vocab_size, freq_cutoff)\n",
        "\n",
        "        return Vocab(src, tgt)\n",
        "\n",
        "    \n",
        "    def save(self, file_path):\n",
        "        json.dump(dict(src_word2id=self.src.word2id, tgt_word2id=self.tgt.word2id), open(file_path, 'w'), indent=2)\n",
        "\n",
        "    \n",
        "    @staticmethod\n",
        "    def load(file_path):\n",
        "        entry = json.load(open(file_path, 'r'))\n",
        "        src_word2id = entry['src_word2id']\n",
        "        tgt_word2id = entry['tgt_word2id']\n",
        "\n",
        "        return Vocab(VocabEntry(src_word2id), VocabEntry(tgt_word2id))\n",
        "    \n",
        "    def __repr__(self):\n",
        "        return 'Vocab(source %d words, target %d words)' % (len(self.src), len(self.tgt))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [
          "model_embeddings.py"
        ],
        "id": "sGUhh2--4FuI"
      },
      "source": [
        "class ModelEmbeddings(nn.Module):\n",
        "    \n",
        "    def __init__(self, word_embed_size: int, vocab: Vocab):\n",
        "        super(ModelEmbeddings, self).__init__()\n",
        "        self.word_embed_size = word_embed_size\n",
        "        \n",
        "        src_pad_token_idx = vocab.src['<pad>']\n",
        "        tgt_pad_token_idx = vocab.tgt['<pad>']\n",
        "        \n",
        "        # A simple lookup table that stores embeddings of a fixed dictionary and size.\n",
        "        # This module is often used to store word embeddings and retrieve them using indices.\n",
        "        # The input to the module is a list of indices, and the output is the corresponding word embeddings.\n",
        "        # padding_idx (int, optional) – If given,\n",
        "        # pads the output with the embedding vector at padding_idx (initialized to zeros)\n",
        "        # whenever it encounters the index.\n",
        "        self.source = nn.Embedding(num_embeddings=len(vocab.src),\n",
        "                                   embedding_dim=word_embed_size,\n",
        "                                   padding_idx=src_pad_token_idx)\n",
        "        self.target = nn.Embedding(len(vocab.tgt), word_embed_size, tgt_pad_token_idx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_UOlS4T4FuI"
      },
      "source": [
        "<div class='alert alert-block alert-info'>\n",
        "<b>nmt_model.py</b>\n",
        "    <ul>\n",
        "        <li><i>Что происходит в форварде?</i> Тупа от начала и до конца: берем слова, переводим, считаем лосс\n",
        "        <li><i>Зачем нам маски?</i> Для инкодера – шобы паддинги не портили нам аттеншн. Для декодера – шобы не считать лосс для паддингов.\n",
        "        <li>Бимсерч\n",
        "        <li>Девайс, загрузка и сохранение модели\n",
        "    </ul>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [
          "nmt_model.py"
        ],
        "id": "rLdU6Xdl4FuJ"
      },
      "source": [
        "class NMT(nn.Module):\n",
        "    \n",
        "    def __init__(self, word_embed_size: int, hidden_size: int, vocab: Vocab, dropout_rate: float = 0.2):\n",
        "        super(NMT, self).__init__()\n",
        "        self.vocab = vocab\n",
        "        self.hidden_size = hidden_size\n",
        "        self.dropout_rate = dropout_rate\n",
        "        \n",
        "        self.model_embeddings = ModelEmbeddings(word_embed_size, vocab)\n",
        "        self.encoder = nn.LSTM(word_embed_size, hidden_size, bidirectional=True)\n",
        "        self.hidden_projection = nn.Linear(2 * hidden_size, hidden_size, bias=False)\n",
        "        self.cell_projection = nn.Linear(2 * hidden_size, hidden_size, bias=False)\n",
        "        \n",
        "        self.decoder = nn.LSTMCell(word_embed_size + hidden_size, hidden_size)\n",
        "        self.att_projection = nn.Linear(2 * hidden_size, hidden_size, bias=False)\n",
        "        self.combined_output_projection = nn.Linear(3 * hidden_size, hidden_size, bias=False)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        \n",
        "        self.target_vocab_projection = nn.Linear(hidden_size, len(vocab.tgt), bias=False)\n",
        "        \n",
        "    @property\n",
        "    def device(self) -> torch.device:\n",
        "        # nn.Embedding.weight\n",
        "        # the learnable weights of the module of shape (src_vocab_size, word_embed_size) initialized from N(0,1)\n",
        "        return self.model_embeddings.source.weight.device\n",
        "    \n",
        "    # Тупа от начала и до конца: берем слова, переводим, считаем лосс\n",
        "    def forward(self, source: List[List[str]], target: List[List[str]]) -> torch.Tensor:\n",
        "        source_lengths = [len(sentence) for sentence in source]\n",
        "        \n",
        "        source_padded = self.vocab.src.to_input_tensor(source, device=self.device)\n",
        "        target_padded = self.vocab.tgt.to_input_tensor(target, device=self.device)\n",
        "\n",
        "        enc_hiddens, dec_init_state = self.encode(source_padded, source_lengths)\n",
        "        enc_masks = self.generate_sent_masks(enc_hiddens, source_lengths)\n",
        "        # (tgt_sentence_len, batch, hidden)\n",
        "        combined_outputs = self.decode(enc_hiddens, dec_init_state, target_padded, enc_masks)\n",
        "        \n",
        "        # predicted log(probability)\n",
        "        # (tgt_sentence_len, batch, tgt_vocab_size)\n",
        "        P = f.log_softmax(self.target_vocab_projection(combined_outputs), dim=-1)\n",
        "        # slice P with indices of gold words\n",
        "        # (tgt_sentence_len, batch, tgt_vocab_size)[(tgt_sentence_length, batch, 1)] = (tgt_sentence_len, batch)\n",
        "        # чтобы не слайсить из словаря <START> вырезаем его [1:]\n",
        "        target_gold_words_log_prob = torch.gather(P, dim=2, index=target_padded[1:].unsqueeze(-1)).squeeze(-1)\n",
        "        # zero out probabilities for <pad> tokens\n",
        "        target_masks = (target_padded != self.vocab.tgt['<pad>']).float()\n",
        "        target_gold_words_log_prob = target_gold_words_log_prob * target_masks[1:]\n",
        "        \n",
        "        return target_gold_words_log_prob.sum(0)\n",
        "        \n",
        "    # Apply the encoder to source sentences to obtain encoder hidden states.\n",
        "    # Additionally, take the final states of the encoder and project them to obtain initial states for decoder.\n",
        "    def encode(self, source_padded: torch.Tensor, source_lengths: List[int]) -> \\\n",
        "            Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n",
        "        # GET WORD EMBEDDINGS\n",
        "        # sentences are lists of indices of words\n",
        "        # (src_sentence_length, batch) –> (src_sentence_length, batch, word_emb_size)\n",
        "        source_emb = self.model_embeddings.source(source_padded)\n",
        "        # packs a Tensor containing padded sequences of variable length.\n",
        "        source_packed = nn.utils.rnn.pack_padded_sequence(source_emb, source_lengths)\n",
        "        \n",
        "        # GET ALL HIDDEN STATES AND FINAL HIDDEN STATE, CELL STATE\n",
        "        enc_hiddens, (hidden_state, cell_state) = self.encoder(source_packed)\n",
        "        # Pads a packed batch of variable length sequences.\n",
        "        # It is an inverse operation to pack_padded_sequence().\n",
        "        # The returned Tensor’s data will be of size T x B x *, where T is the length of the longest\n",
        "        # (batch, src_sentence_length, hidden_size)\n",
        "        enc_hiddens, lens_unpacked = nn.utils.rnn.pad_packed_sequence(enc_hiddens, batch_first=True)\n",
        "        \n",
        "        # INITIALIZE DECODER INITIAL STATES\n",
        "        # first, concatenate hidden_forward and hidden_backward\n",
        "        # (forward + backward = 2, batch, hidden_size) –> (batch, 2 * hidden_size)\n",
        "        # первый для бэкварда – последний для форварда, последний для форварда – первый для бэкварда\n",
        "        # то есть это последние хидден стейты, t = seq_len\n",
        "        hidden_concat = torch.cat((hidden_state[0], hidden_state[1]), dim=1)\n",
        "        dec_init_hidden = self.hidden_projection(hidden_concat)\n",
        "        \n",
        "        cell_concat = torch.cat((cell_state[0], cell_state[1]), dim=1)\n",
        "        dec_init_cell = self.cell_projection(cell_concat)\n",
        "        \n",
        "        return enc_hiddens, (dec_init_hidden, dec_init_cell)\n",
        "    \n",
        "    def decode(self,\n",
        "               enc_hiddens: torch.Tensor,\n",
        "               dec_init_state: Tuple[torch.Tensor, torch.Tensor],\n",
        "               target_padded: torch.Tensor,\n",
        "               enc_masks: torch.Tensor) -> torch.Tensor:\n",
        "        \n",
        "        hidden_state, cell_state = dec_init_state\n",
        "\n",
        "        # (batch, src_sentence_length, 2 * hidden_size) –> (batch, src_sentence_length, hidden_size)\n",
        "        enc_hiddens_proj = self.att_projection(enc_hiddens)\n",
        "        \n",
        "        # у того, что есть результат нашей модели, мы чопаем <END>\n",
        "        # (tgt_sentence_length, batch) –> (tgt_sentence_length, batch, word_emb_size)\n",
        "        target_emb = self.model_embeddings.target(target_padded[:-1])  # ???\n",
        "        # initialize o_0\n",
        "        # (batch, hidden_size)\n",
        "        combined_output = torch.zeros(target_emb.size(1), self.hidden_size, device=self.device)\n",
        "        combined_outputs = []\n",
        "        # iterate over words in sentences\n",
        "        for tgt_emb in torch.split(target_emb, 1):\n",
        "            # [(1, batch, word_emb_size), (batch, hidden_size)]\n",
        "            tgt_emb = torch.cat((tgt_emb.squeeze(0), combined_output), 1)\n",
        "            # (batch, hidden_size)\n",
        "            (hidden_state, cell_state) = self.decoder(tgt_emb, (hidden_state, cell_state))\n",
        "            \n",
        "            # ATTENTION\n",
        "            # If input is a (b×n×m) tensor, mat2 is a (b×m×p) tensor, out will be a (b×n×p) tensor.\n",
        "            # (batch, src_sentence_len, hidden) x (batch, hidden, 1) = (batch, src_sentence_length)\n",
        "            e_t = torch.bmm(enc_hiddens_proj, hidden_state.unsqueeze(2)).squeeze(2)\n",
        "            \n",
        "            # set e_t to -inf where the word is pad in order to make softmax equal 0\n",
        "            if enc_masks is not None:\n",
        "                e_t.masked_fill_(enc_masks.bool(), -float('inf'))\n",
        "                \n",
        "            alpha_t = f.softmax(e_t, 1)\n",
        "            # <(batch, 1, src_sentence_len), (batch, src_sentence_len, 2 * hidden)> = (batch, 2 * hidden)\n",
        "            a_t = torch.bmm(alpha_t.unsqueeze(1), enc_hiddens).squeeze(1)\n",
        "            # [(batch, 2 * hidden), (batch, hidden_size)] = (batch, 3 * hidden_size)\n",
        "            u_t = torch.cat((a_t, hidden_state), 1)\n",
        "            # (batch, hidden)\n",
        "            v_t = self.combined_output_projection(u_t)\n",
        "            combined_output = self.dropout(torch.tanh(v_t))\n",
        "            combined_outputs.append(combined_output)\n",
        "        \n",
        "        combined_outputs = torch.stack(combined_outputs)\n",
        "        return combined_outputs\n",
        "    \n",
        "    # The generate sent masks() function produces a tensor called enc masks.\n",
        "    # It has shape (batch size, max source sentence length)\n",
        "    # and contains 1s in positions corresponding to ‘pad’ tokens in the input, and 0s for non-pad tokens.\n",
        "    # Masks are used during the attention computation.\n",
        "    def generate_sent_masks(self, enc_hiddens: torch.Tensor, source_lengths: List[int]) -> torch.Tensor:\n",
        "        enc_masks = torch.zeros(enc_hiddens.size(0), enc_hiddens.size(1), dtype= torch.float)\n",
        "        for enc_hidden, src_sentence_len in enumerate(source_lengths):\n",
        "            enc_masks[enc_hidden, src_sentence_len:] = 1\n",
        "        return enc_masks.to(self.device)\n",
        "    \n",
        "    # ЛЕНЬ СУКА\n",
        "    # step нужен для beam_search\n",
        "    def step(self, Ybar_t: torch.Tensor,\n",
        "            dec_state: Tuple[torch.Tensor, torch.Tensor],\n",
        "            enc_hiddens: torch.Tensor,\n",
        "            enc_hiddens_proj: torch.Tensor,\n",
        "            enc_masks: torch.Tensor) -> Tuple[Tuple, torch.Tensor, torch.Tensor]:\n",
        "\n",
        "        combined_output = None\n",
        "\n",
        "        # YOUR CODE HERE (~3 Lines)\n",
        "        dec_state = self.decoder(Ybar_t, dec_state)\n",
        "        dec_hidden, dec_cell = dec_state[0], dec_state[1]\n",
        "        e_t = torch.squeeze(torch.bmm(enc_hiddens_proj, torch.unsqueeze(dec_hidden, dim=2)), dim=2)\n",
        "        # END YOUR CODE\n",
        "\n",
        "        # Set e_t to -inf where enc_masks has 1\n",
        "        if enc_masks is not None:\n",
        "            e_t.data.masked_fill_(enc_masks.bool(), -float('inf'))\n",
        "\n",
        "        # YOUR CODE HERE (~6 Lines)\n",
        "        alpha_t = f.softmax(e_t, dim=1)\n",
        "        a_t = torch.squeeze(torch.bmm(torch.unsqueeze(alpha_t, dim=1), enc_hiddens), dim=1)\n",
        "        U_t = torch.cat((a_t, dec_hidden), dim=1)\n",
        "        V_t = self.combined_output_projection(U_t)\n",
        "        O_t = self.dropout(torch.tanh(V_t))\n",
        "        # END YOUR CODE\n",
        "\n",
        "        combined_output = O_t\n",
        "        return dec_state, combined_output, e_t\n",
        "\n",
        "    def beam_search(self, src_sent: List[str], beam_size: int=5, max_decoding_time_step: int=70) -> List[Hypothesis]:\n",
        "        src_sents_var = self.vocab.src.to_input_tensor([src_sent], self.device)\n",
        "\n",
        "        src_encodings, dec_init_vec = self.encode(src_sents_var, [len(src_sent)])\n",
        "        src_encodings_att_linear = self.att_projection(src_encodings)\n",
        "\n",
        "        h_tm1 = dec_init_vec\n",
        "        att_tm1 = torch.zeros(1, self.hidden_size, device=self.device)\n",
        "\n",
        "        eos_id = self.vocab.tgt['</s>']\n",
        "\n",
        "        hypotheses = [['<s>']]\n",
        "        hyp_scores = torch.zeros(len(hypotheses), dtype=torch.float, device=self.device)\n",
        "        completed_hypotheses = []\n",
        "\n",
        "        t = 0\n",
        "        while len(completed_hypotheses) < beam_size and t < max_decoding_time_step:\n",
        "            t += 1\n",
        "            hyp_num = len(hypotheses)\n",
        "\n",
        "            exp_src_encodings = src_encodings.expand(hyp_num,\n",
        "                                                     src_encodings.size(1),\n",
        "                                                     src_encodings.size(2))\n",
        "\n",
        "            exp_src_encodings_att_linear = src_encodings_att_linear.expand(hyp_num,\n",
        "                                                                           src_encodings_att_linear.size(1),\n",
        "                                                                           src_encodings_att_linear.size(2))\n",
        "\n",
        "            y_tm1 = torch.tensor([self.vocab.tgt[hyp[-1]] for hyp in hypotheses], dtype=torch.long, device=self.device)\n",
        "            y_t_embed = self.model_embeddings.target(y_tm1)\n",
        "\n",
        "            x = torch.cat([y_t_embed, att_tm1], dim=-1)\n",
        "\n",
        "            (h_t, cell_t), att_t, _ = self.step(x, h_tm1,\n",
        "                                                exp_src_encodings, exp_src_encodings_att_linear, enc_masks=None)\n",
        "\n",
        "            # log probabilities over target words\n",
        "            log_p_t = f.log_softmax(self.target_vocab_projection(att_t), dim=-1)\n",
        "\n",
        "            live_hyp_num = beam_size - len(completed_hypotheses)\n",
        "            contiuating_hyp_scores = (hyp_scores.unsqueeze(1).expand_as(log_p_t) + log_p_t).view(-1)\n",
        "            top_cand_hyp_scores, top_cand_hyp_pos = torch.topk(contiuating_hyp_scores, k=live_hyp_num)\n",
        "\n",
        "            prev_hyp_ids = top_cand_hyp_pos // len(self.vocab.tgt)\n",
        "            hyp_word_ids = top_cand_hyp_pos % len(self.vocab.tgt)\n",
        "\n",
        "            new_hypotheses = []\n",
        "            live_hyp_ids = []\n",
        "            new_hyp_scores = []\n",
        "\n",
        "            for prev_hyp_id, hyp_word_id, cand_new_hyp_score in zip(prev_hyp_ids, hyp_word_ids, top_cand_hyp_scores):\n",
        "                prev_hyp_id = prev_hyp_id.item()\n",
        "                hyp_word_id = hyp_word_id.item()\n",
        "                cand_new_hyp_score = cand_new_hyp_score.item()\n",
        "\n",
        "                hyp_word = self.vocab.tgt.id2word[hyp_word_id]\n",
        "                new_hyp_sent = hypotheses[prev_hyp_id] + [hyp_word]\n",
        "                if hyp_word == '</s>':\n",
        "                    completed_hypotheses.append(Hypothesis(value=new_hyp_sent[1:-1],\n",
        "                                                           score=cand_new_hyp_score))\n",
        "                else:\n",
        "                    new_hypotheses.append(new_hyp_sent)\n",
        "                    live_hyp_ids.append(prev_hyp_id)\n",
        "                    new_hyp_scores.append(cand_new_hyp_score)\n",
        "\n",
        "            if len(completed_hypotheses) == beam_size:\n",
        "                break\n",
        "\n",
        "            live_hyp_ids = torch.tensor(live_hyp_ids, dtype=torch.long, device=self.device)\n",
        "            h_tm1 = (h_t[live_hyp_ids], cell_t[live_hyp_ids])\n",
        "            att_tm1 = att_t[live_hyp_ids]\n",
        "\n",
        "            hypotheses = new_hypotheses\n",
        "            hyp_scores = torch.tensor(new_hyp_scores, dtype=torch.float, device=self.device)\n",
        "\n",
        "        if len(completed_hypotheses) == 0:\n",
        "            completed_hypotheses.append(Hypothesis(value=hypotheses[0][1:],\n",
        "                                                   score=hyp_scores[0].item()))\n",
        "\n",
        "        completed_hypotheses.sort(key=lambda hyp: hyp.score, reverse=True)\n",
        "\n",
        "        return completed_hypotheses\n",
        "    \n",
        "    def load(model_path: str):\n",
        "        params = torch.load(model_path, map_location=lambda storage, loc: storage)\n",
        "        args = params['args']\n",
        "        model = NMT(vocab=params['vocab'], **args)\n",
        "        model.load_state_dict(params['state_dict'])\n",
        "\n",
        "        return model\n",
        "    \n",
        "    def save(self, path: str):\n",
        "        print('save model parameters to [%s]' % path, file=sys.stderr)\n",
        "\n",
        "        params = {\n",
        "            'args': dict(\n",
        "                word_embed_size=self.model_embeddings.word_embed_size,\n",
        "                hidden_size=self.hidden_size,\n",
        "                dropout_rate=self.dropout_rate),\n",
        "            'vocab': self.vocab,\n",
        "            'state_dict': self.state_dict()\n",
        "        }\n",
        "\n",
        "        torch.save(params, path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TYpJydK4FuK"
      },
      "source": [
        "<div class='alert alert-block alert-info'>\n",
        "<b>run.py</b><br>\n",
        "<b>TRAIN</b>\n",
        "    <ul>\n",
        "        <li><i>Что значит cuda?</i> Значит на ГПУ\n",
        "        <li><i>Как работает yield?</i>\n",
        "        <li><i>Как запускать?</i> Да тупа вызвать функцию мэйн, только параметры надо передать сразу.\n",
        "    </ul>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-CyA6X1X4FuL"
      },
      "source": [
        "def read_corpus(path: str, source: str) -> List[List]:\n",
        "    data = []\n",
        "    for line in open(path):\n",
        "        sentence = nltk.word_tokenize(line)\n",
        "        if source == 'tgt':\n",
        "            sentence = ['<s>'] + sentence + ['</s>']\n",
        "        data.append(sentence)\n",
        "    \n",
        "    return data\n",
        "\n",
        "\n",
        "def batch_iter(data: List[Tuple], batch_size: int, shuffle: bool = False):\n",
        "    batch_num = math.ceil(len(data) / batch_size)  # количество батчей\n",
        "    indices = list(range(len(data)))\n",
        "    \n",
        "    if shuffle:\n",
        "        np.random.shuffle(indices)\n",
        "    \n",
        "    for i in range(batch_num):\n",
        "        indices_i = indices[batch_size * i:batch_size * (i + 1)]\n",
        "        sents = [data[idx] for idx in indices_i]\n",
        "        \n",
        "        sents = sorted(sents, key=lambda s: len(s[0]), reverse=True)\n",
        "        \n",
        "        src_sents = [s[0] for s in sents]\n",
        "        tgt_sents = [s[1] for s in sents]\n",
        "        \n",
        "        yield src_sents, tgt_sents"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MY3taPra4FuM"
      },
      "source": [
        "def train(args: Dict):\n",
        "    # READ PARALLEL CORPORA\n",
        "    train_data_src = read_corpus(args['--train-src'], source='src')\n",
        "    train_data_tgt = read_corpus(args['--train-tgt'], source='tgt')\n",
        "    dev_data_src = read_corpus(args['--dev-src'], source='src')\n",
        "    dev_data_tgt = read_corpus(args['--dev-tgt'], source='tgt')\n",
        "\n",
        "    train_data = list(zip(train_data_src, train_data_tgt))\n",
        "    dev_data = list(zip(dev_data_src, dev_data_tgt))\n",
        "\n",
        "    # READ PARAMETERS\n",
        "    train_batch_size = int(args['--batch-size'])\n",
        "    clip_grad = float(args['--clip-grad'])\n",
        "    valid_niter = int(args['--valid-niter'])  # perform validation after how many iterations [default: 2000]\n",
        "    log_every = int(args['--log-every'])\n",
        "    model_save_path = args['--save-to']\n",
        "\n",
        "    # READ VOCABULARY\n",
        "    vocab = Vocab.load(args['--vocab'])\n",
        "\n",
        "    # INITIALIZE MODEL\n",
        "    model = NMT(\n",
        "        word_embed_size=int(args['--embed-size']),\n",
        "        hidden_size=int(args['--hidden-size']),\n",
        "        dropout_rate=float(args['--dropout']),\n",
        "        vocab=vocab\n",
        "    )\n",
        "    # Set the module in training mode.\n",
        "    model.train()\n",
        "    \n",
        "    # INITIALIZE PARAMETERS\n",
        "    uniform_init = float(args['--uniform-init'])  # uniformly initialize all parameters [default: 0.1]\n",
        "    if np.abs(uniform_init) > 0.:\n",
        "        print('uniformly initialize parameters [-%f, +%f]' % (uniform_init, uniform_init), file=sys.stderr)\n",
        "        for p in model.parameters():\n",
        "            p.data.uniform_(-uniform_init, uniform_init)\n",
        "\n",
        "    ##### ВОТ ТУТ НЕ ПОНЕЛ #####\n",
        "    vocab_mask = torch.ones(len(vocab.tgt))\n",
        "    vocab_mask[vocab.tgt['<pad>']] = 0\n",
        "\n",
        "    device = torch.device(\"cuda:0\" if args['--cuda'] else \"cpu\")\n",
        "    print('use device: %s' % device, file=sys.stderr)\n",
        "    # Moves and/or casts the parameters and buffers.\n",
        "    model = model.to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=float(args['--lr']))\n",
        "\n",
        "    # INITIALIZE VARIABLES DESCRIBING TRAINING\n",
        "    num_trial = 0\n",
        "    train_iter = patience = cum_loss = report_loss = cum_tgt_words = report_tgt_words = 0\n",
        "    cum_examples = report_examples = epoch = valid_num = 0\n",
        "    hist_valid_scores = []\n",
        "    train_time = begin_time = time.time()\n",
        "    print('begin Maximum Likelihood training')\n",
        "    \n",
        "    # TRAIN\n",
        "    while True:\n",
        "        epoch += 1\n",
        "        for src_sents, tgt_sents in batch_iter(train_data, batch_size=train_batch_size, shuffle=True):\n",
        "            # ОБУЧЕНИЕ\n",
        "            train_iter += 1\n",
        "\n",
        "            # initialize gradients\n",
        "            optimizer.zero_grad()\n",
        "            batch_size = len(src_sents)\n",
        "            # calc losses for every sentence\n",
        "            example_losses = -model(src_sents, tgt_sents)  # (batch_size,)\n",
        "            # sum it up\n",
        "            batch_loss = example_losses.sum()\n",
        "            # average loss\n",
        "            loss = batch_loss / batch_size\n",
        "            # CALCULATE GRADIENT\n",
        "            loss.backward()\n",
        "            # clip gradient\n",
        "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad)\n",
        "            # UPDATE PARAMETERS\n",
        "            optimizer.step()\n",
        "\n",
        "            # ЛОГИ\n",
        "            batch_losses_val = batch_loss.item()\n",
        "            report_loss += batch_losses_val\n",
        "            cum_loss += batch_losses_val\n",
        "\n",
        "            tgt_words_num_to_predict = sum(len(s[1:]) for s in tgt_sents)  # omitting leading `<s>`\n",
        "            report_tgt_words += tgt_words_num_to_predict\n",
        "            cum_tgt_words += tgt_words_num_to_predict\n",
        "            report_examples += batch_size\n",
        "            cum_examples += batch_size\n",
        "            \n",
        "            if train_iter % log_every == 0:\n",
        "                # print('epoch %d, iter %d, avg. loss %.2f, avg. ppl %.2f ' \\\n",
        "                #       'cum. examples %d, speed %.2f words/sec, time elapsed %.2f sec' % (\n",
        "                #     epoch, train_iter, report_loss / report_examples, math.exp(report_loss / report_tgt_words),\n",
        "                #     cum_examples, report_tgt_words / (time.time() - train_time), time.time() - begin_time\n",
        "                # ))\n",
        "                wandb.log({\"loss\":report_loss / report_examples, \"ppl\": math.exp(report_loss / report_tgt_words)})\n",
        "\n",
        "                train_time = time.time()\n",
        "                report_loss = report_tgt_words = report_examples = 0.\n",
        "                \n",
        "            # ВАЛИДАЦИЯ\n",
        "            if train_iter % valid_niter == 0:\n",
        "                print('epoch %d, iter %d, cum. loss %.2f, cum. ppl %.2f cum. examples %d' % (\n",
        "                    epoch, train_iter, cum_loss / cum_examples, np.exp(cum_loss / cum_tgt_words), cum_examples\n",
        "                ))\n",
        "\n",
        "                cum_loss = cum_examples = cum_tgt_words = 0.\n",
        "                valid_num += 1\n",
        "\n",
        "                print('begin validation ...')\n",
        "\n",
        "                # compute dev. ppl and bleu\n",
        "                dev_ppl = evaluate_ppl(model, dev_data, batch_size=128)   # dev batch size can be a bit larger\n",
        "                valid_metric = -dev_ppl\n",
        "\n",
        "                print('validation: iter %d, dev. ppl %f' % (train_iter, dev_ppl))\n",
        "\n",
        "                is_better = len(hist_valid_scores) == 0 or valid_metric > max(hist_valid_scores)\n",
        "                hist_valid_scores.append(valid_metric)\n",
        "\n",
        "                if is_better:\n",
        "                    patience = 0\n",
        "                    print('save currently the best model to [%s]' % model_save_path)\n",
        "                    model.save(model_save_path)\n",
        "\n",
        "                    # also save the optimizers' state\n",
        "                    torch.save(optimizer.state_dict(), model_save_path + '.optim')\n",
        "                elif patience < int(args['--patience']):\n",
        "                    patience += 1\n",
        "                    print('hit patience %d' % patience, file=sys.stderr)\n",
        "\n",
        "                    if patience == int(args['--patience']):\n",
        "                        num_trial += 1\n",
        "                        print('hit #%d trial' % num_trial, file=sys.stderr)\n",
        "                        if num_trial == int(args['--max-num-trial']):\n",
        "                            print('early stop!', file=sys.stderr)\n",
        "                            exit(0)\n",
        "\n",
        "                        # decay lr, and restore from previously best checkpoint\n",
        "                        lr = optimizer.param_groups[0]['lr'] * float(args['--lr-decay'])\n",
        "                        print('load previously best model and decay learning rate to %f' % lr)\n",
        "\n",
        "                        # LOAD\n",
        "                        params = torch.load(model_save_path, map_location=lambda storage, loc: storage)\n",
        "                        model.load_state_dict(params['state_dict'])\n",
        "                        model = model.to(device)\n",
        "\n",
        "                        print('restore parameters of the optimizers')\n",
        "                        optimizer.load_state_dict(torch.load(model_save_path + '.optim'))\n",
        "\n",
        "                        # set new lr\n",
        "                        for param_group in optimizer.param_groups:\n",
        "                            param_group['lr'] = lr\n",
        "\n",
        "                        # reset patience\n",
        "                        patience = 0\n",
        "                if epoch == int(args['--max-epoch']):\n",
        "                    print('reached maximum number of epochs!')\n",
        "                    exit(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQYRLP5p4FuO"
      },
      "source": [
        "def evaluate_ppl(model, dev_data, batch_size=32):\n",
        "    # save the previous mode and turn to evaluation mode\n",
        "    was_training = model.training\n",
        "    model.eval()\n",
        "\n",
        "    cum_loss = 0.\n",
        "    cum_tgt_words = 0.\n",
        "\n",
        "    # no_grad() signals backend to throw away all gradients\n",
        "    with torch.no_grad():\n",
        "        for src_sents, tgt_sents in batch_iter(dev_data, batch_size):\n",
        "            # __call__ == forward\n",
        "            loss = -model(src_sents, tgt_sents).sum()\n",
        "\n",
        "            cum_loss += loss.item()\n",
        "            tgt_word_num_to_predict = sum(len(s[1:]) for s in tgt_sents)  # omitting leading `<s>`\n",
        "            cum_tgt_words += tgt_word_num_to_predict\n",
        "\n",
        "        ppl = np.exp(cum_loss / cum_tgt_words)\n",
        "\n",
        "    # return previous mode\n",
        "    if was_training:\n",
        "        model.train()\n",
        "\n",
        "    return ppl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0giPBQm-4FuO"
      },
      "source": [
        "def compute_corpus_level_bleu_score(references: List[List[str]], hypotheses: List[Hypothesis]) -> float:\n",
        "    # cut off <s> and </s>\n",
        "    if references[0][0] == '<s>':\n",
        "        references = [ref[1:-1] for ref in references]\n",
        "    bleu_score = corpus_bleu([[ref] for ref in references],\n",
        "                             [hyp.value for hyp in hypotheses])\n",
        "    return bleu_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ud_h5tpI6SMY"
      },
      "source": [
        "<div class='alert alert-block alert-info'>\n",
        "<b>run.py</b><br>\n",
        "<b>TEST</b>\n",
        "    <ul>\n",
        "        <li><i>Что значит cuda?</i> Значит на ГПУ\n",
        "        <li><i>Как работает yield?</i>\n",
        "        <li><i>Как запускать?</i> Да тупа вызвать функцию мэйн, только параметры надо передать сразу.\n",
        "    </ul>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0UNvqPI4FuO"
      },
      "source": [
        "# TEST MODEL\n",
        "# тут пример создания новой модели\n",
        "def decode(args: Dict[str, str]):\n",
        "    print(\"load test source sentences from [{}]\".format(args['TEST_SOURCE_FILE']))\n",
        "    test_data_src = read_corpus(args['TEST_SOURCE_FILE'], source='src')\n",
        "    if args['TEST_TARGET_FILE']:\n",
        "        print(\"load test target sentences from [{}]\".format(args['TEST_TARGET_FILE']))\n",
        "        test_data_tgt = read_corpus(args['TEST_TARGET_FILE'], source='tgt')\n",
        "\n",
        "    # we create new model\n",
        "    print(\"load model from {}\".format(args['MODEL_PATH']))\n",
        "    model = NMT.load(args['MODEL_PATH'])\n",
        "\n",
        "    if args['--cuda']:\n",
        "        model = model.to(torch.device(\"cuda:0\"))\n",
        "\n",
        "    hypotheses = beam_search(model, test_data_src,\n",
        "                             beam_size=int(args['--beam-size']),\n",
        "                             max_decoding_time_step=int(args['--max-decoding-time-step']))\n",
        "\n",
        "    if args['TEST_TARGET_FILE']:\n",
        "        top_hypotheses = [hyps[0] for hyps in hypotheses]\n",
        "        bleu_score = compute_corpus_level_bleu_score(test_data_tgt, top_hypotheses)\n",
        "        print('Corpus BLEU: {}'.format(bleu_score * 100))\n",
        "\n",
        "    with open(args['OUTPUT_FILE'], 'w') as f:\n",
        "        for src_sent, hyps in zip(test_data_src, hypotheses):\n",
        "            top_hyp = hyps[0]\n",
        "            hyp_sent = ' '.join(top_hyp.value)\n",
        "            f.write(hyp_sent + '\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iDgfplLY4FuO"
      },
      "source": [
        "def beam_search(model: NMT, test_data_src: List[List[str]], beam_size: int, max_decoding_time_step: int) -> List[List[Hypothesis]]:\n",
        "    training_mode = model.training  # returns True if model was in a training mode\n",
        "    model.eval()  # turn to evaluation mode\n",
        "\n",
        "    # generate hypotheses\n",
        "    hypotheses = []\n",
        "    \n",
        "    # Disabling gradient calculation is useful for inference,\n",
        "    # when you are sure that you will not call Tensor.backward().\n",
        "    # It will reduce memory consumption for computations that would otherwise have requires_grad=True.\n",
        "    with torch.no_grad():\n",
        "        for src_sent in tqdm(test_data_src, desc='Decoding', file=sys.stdout):\n",
        "            # call internal beam_search\n",
        "            example_hyps = model.beam_search(\n",
        "                src_sent, beam_size=beam_size, max_decoding_time_step=max_decoding_time_step\n",
        "            )\n",
        "            \n",
        "            # add a new generated hypotheses for current source sentence\n",
        "            hypotheses.append(example_hyps)\n",
        "\n",
        "    # turn the mode back\n",
        "    if training_mode:\n",
        "        model.train(training_mode)\n",
        "\n",
        "    return hypotheses"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xo-NZZO64FuP"
      },
      "source": [
        "# вызов train_local\n",
        "\n",
        "# !python main train\n",
        "# --train-src=/Users/ainurmukh/nn_course/a5_public/en_es_data/train.es\n",
        "# --train-tgt=/Users/ainurmukh/nn_course/a5_public/en_es_data/train.en\n",
        "# --dev-src=/Users/ainurmukh/nn_course/a5_public/en_es_data/dev.es\n",
        "# --dev-tgt=/Users/ainurmukh/nn_course/a5_public/en_es_data/dev.en\n",
        "# --vocab=vocab.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACltmPrW4FuP"
      },
      "source": [
        "# ВОТ ТАКАЯ ШТУКА В МЭЙНЕ\n",
        "\n",
        "def main(args):\n",
        "    # читаем аргументы из ввода\n",
        "    # args = docopt(__doc__)\n",
        "\n",
        "    seed = int(args['--seed'])\n",
        "    torch.manual_seed(seed)\n",
        "    if args['--cuda']:\n",
        "        torch.cuda.manual_seed(seed)\n",
        "    np.random.seed(seed * 13 // 7)\n",
        "\n",
        "    if args['train']:\n",
        "        train(args)\n",
        "    elif args['decode']:\n",
        "        decode(args)\n",
        "    else:\n",
        "        raise RuntimeError('invalid run mode')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LcVJKXQL4FuP"
      },
      "source": [
        "train_args = {\n",
        " '--batch-size': '32',\n",
        " '--beam-size': '5',\n",
        " '--clip-grad': '5.0',\n",
        " '--cuda': True,\n",
        " '--dev-src': '/content/gdrive/MyDrive/Colab/a4/en_es_data/dev.es',\n",
        " '--dev-tgt': '/content/gdrive/MyDrive/Colab/a4/en_es_data/dev.en',\n",
        " '--dropout': '0.3',\n",
        " '--embed-size': '256',\n",
        " '--help': False,\n",
        " '--hidden-size': '256',\n",
        " '--input-feed': False,\n",
        " '--log-every': '10',\n",
        " '--lr': '0.001',\n",
        " '--lr-decay': '0.5',\n",
        " '--max-decoding-time-step': '70',\n",
        " '--max-epoch': '30',\n",
        " '--max-num-trial': '5',\n",
        " '--patience': '5',\n",
        " '--sample-size': '5',\n",
        " '--save-to': '/content/gdrive/MyDrive/Colab/model.bin',\n",
        " '--seed': '0',\n",
        " '--train-src': '/content/gdrive/MyDrive/Colab/a4/en_es_data/train.es',\n",
        " '--train-tgt': '/content/gdrive/MyDrive/Colab/a4/en_es_data/train.en',\n",
        " '--uniform-init': '0.1',\n",
        " '--valid-niter': '2000',\n",
        " '--vocab': '/content/gdrive/MyDrive/Colab/a4/vocab.json',\n",
        " 'MODEL_PATH': None,\n",
        " 'OUTPUT_FILE': None,\n",
        " 'TEST_SOURCE_FILE': None,\n",
        " 'TEST_TARGET_FILE': None,\n",
        " 'decode': False,\n",
        " 'train': True}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "id": "_Yca8gBn_LHO",
        "outputId": "ee909011-deb1-4fba-b115-a71809f2a8bb"
      },
      "source": [
        "wandb.init(project=\"test-drive\", config={\n",
        "    \"learning_rate\": 0.001,\n",
        "    \"dropout\": 0.3,\n",
        "    \"architecture\": \"NMT_no_decoder\",\n",
        "})\n",
        "config = wandb.config"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mainurmukh\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.10.12<br/>\n",
              "                Syncing run <strong style=\"color:#cdcd00\">electric-fire-1</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/ainurmukh/test-drive\" target=\"_blank\">https://wandb.ai/ainurmukh/test-drive</a><br/>\n",
              "                Run page: <a href=\"https://wandb.ai/ainurmukh/test-drive/runs/2vm2zfvl\" target=\"_blank\">https://wandb.ai/ainurmukh/test-drive/runs/2vm2zfvl</a><br/>\n",
              "                Run data is saved locally in <code>/content/wandb/run-20210108_075436-2vm2zfvl</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2Kd1u6Np4FuQ",
        "outputId": "c5778e72-67ae-49c4-f12d-b0b88ce88a5d"
      },
      "source": [
        "main(train_args)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "uniformly initialize parameters [-0.100000, +0.100000]\n",
            "use device: cuda:0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "begin Maximum Likelihood training\n",
            "epoch 1, iter 2000, cum. loss 96.27, cum. ppl 107.68 cum. examples 64000\n",
            "begin validation ...\n",
            "validation: iter 2000, dev. ppl 33.845637\n",
            "save currently the best model to [/content/gdrive/MyDrive/Colab/model.bin]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "save model parameters to [/content/gdrive/MyDrive/Colab/model.bin]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 1, iter 4000, cum. loss 67.31, cum. ppl 26.30 cum. examples 64000\n",
            "begin validation ...\n",
            "validation: iter 4000, dev. ppl 16.583964\n",
            "save currently the best model to [/content/gdrive/MyDrive/Colab/model.bin]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "save model parameters to [/content/gdrive/MyDrive/Colab/model.bin]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 1, iter 6000, cum. loss 57.73, cum. ppl 16.51 cum. examples 64000\n",
            "begin validation ...\n",
            "validation: iter 6000, dev. ppl 12.357079\n",
            "save currently the best model to [/content/gdrive/MyDrive/Colab/model.bin]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "save model parameters to [/content/gdrive/MyDrive/Colab/model.bin]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 2, iter 8000, cum. loss 50.80, cum. ppl 11.84 cum. examples 63977\n",
            "begin validation ...\n",
            "validation: iter 8000, dev. ppl 10.633495\n",
            "save currently the best model to [/content/gdrive/MyDrive/Colab/model.bin]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "save model parameters to [/content/gdrive/MyDrive/Colab/model.bin]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 2, iter 10000, cum. loss 47.73, cum. ppl 10.20 cum. examples 64000\n",
            "begin validation ...\n",
            "validation: iter 10000, dev. ppl 9.691170\n",
            "save currently the best model to [/content/gdrive/MyDrive/Colab/model.bin]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "save model parameters to [/content/gdrive/MyDrive/Colab/model.bin]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 2, iter 12000, cum. loss 46.28, cum. ppl 9.48 cum. examples 64000\n",
            "begin validation ...\n",
            "validation: iter 12000, dev. ppl 8.854040\n",
            "save currently the best model to [/content/gdrive/MyDrive/Colab/model.bin]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "save model parameters to [/content/gdrive/MyDrive/Colab/model.bin]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 3, iter 14000, cum. loss 44.28, cum. ppl 8.47 cum. examples 63977\n",
            "begin validation ...\n",
            "validation: iter 14000, dev. ppl 8.421873\n",
            "save currently the best model to [/content/gdrive/MyDrive/Colab/model.bin]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "save model parameters to [/content/gdrive/MyDrive/Colab/model.bin]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 3, iter 16000, cum. loss 40.17, cum. ppl 7.05 cum. examples 64000\n",
            "begin validation ...\n",
            "validation: iter 16000, dev. ppl 8.166235\n",
            "save currently the best model to [/content/gdrive/MyDrive/Colab/model.bin]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "save model parameters to [/content/gdrive/MyDrive/Colab/model.bin]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 3, iter 18000, cum. loss 40.45, cum. ppl 7.10 cum. examples 64000\n",
            "begin validation ...\n",
            "validation: iter 18000, dev. ppl 7.978001\n",
            "save currently the best model to [/content/gdrive/MyDrive/Colab/model.bin]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "save model parameters to [/content/gdrive/MyDrive/Colab/model.bin]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 3, iter 20000, cum. loss 40.36, cum. ppl 7.11 cum. examples 64000\n",
            "begin validation ...\n",
            "validation: iter 20000, dev. ppl 7.628397\n",
            "save currently the best model to [/content/gdrive/MyDrive/Colab/model.bin]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "save model parameters to [/content/gdrive/MyDrive/Colab/model.bin]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 4, iter 22000, cum. loss 36.33, cum. ppl 5.83 cum. examples 63977\n",
            "begin validation ...\n",
            "validation: iter 22000, dev. ppl 7.611771\n",
            "save currently the best model to [/content/gdrive/MyDrive/Colab/model.bin]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "save model parameters to [/content/gdrive/MyDrive/Colab/model.bin]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 4, iter 24000, cum. loss 36.28, cum. ppl 5.84 cum. examples 64000\n",
            "begin validation ...\n",
            "validation: iter 24000, dev. ppl 7.630467\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "hit patience 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 4, iter 26000, cum. loss 36.86, cum. ppl 5.99 cum. examples 64000\n",
            "begin validation ...\n",
            "validation: iter 26000, dev. ppl 7.508859\n",
            "save currently the best model to [/content/gdrive/MyDrive/Colab/model.bin]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "save model parameters to [/content/gdrive/MyDrive/Colab/model.bin]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 5, iter 28000, cum. loss 34.92, cum. ppl 5.45 cum. examples 63977\n",
            "begin validation ...\n",
            "validation: iter 28000, dev. ppl 7.662211\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "hit patience 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 5, iter 30000, cum. loss 33.37, cum. ppl 5.06 cum. examples 64000\n",
            "begin validation ...\n",
            "validation: iter 30000, dev. ppl 7.453411\n",
            "save currently the best model to [/content/gdrive/MyDrive/Colab/model.bin]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "save model parameters to [/content/gdrive/MyDrive/Colab/model.bin]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 5, iter 32000, cum. loss 34.11, cum. ppl 5.23 cum. examples 64000\n",
            "begin validation ...\n",
            "validation: iter 32000, dev. ppl 7.411273\n",
            "save currently the best model to [/content/gdrive/MyDrive/Colab/model.bin]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "save model parameters to [/content/gdrive/MyDrive/Colab/model.bin]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 6, iter 34000, cum. loss 34.16, cum. ppl 5.27 cum. examples 63977\n",
            "begin validation ...\n",
            "validation: iter 34000, dev. ppl 7.443300\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "hit patience 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 6, iter 36000, cum. loss 30.79, cum. ppl 4.45 cum. examples 64000\n",
            "begin validation ...\n",
            "validation: iter 36000, dev. ppl 7.580232\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "hit patience 2\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 6, iter 38000, cum. loss 31.82, cum. ppl 4.69 cum. examples 64000\n",
            "begin validation ...\n",
            "validation: iter 38000, dev. ppl 7.361073\n",
            "save currently the best model to [/content/gdrive/MyDrive/Colab/model.bin]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "save model parameters to [/content/gdrive/MyDrive/Colab/model.bin]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 6, iter 40000, cum. loss 32.38, cum. ppl 4.83 cum. examples 64000\n",
            "begin validation ...\n",
            "validation: iter 40000, dev. ppl 7.464431\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "hit patience 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 7, iter 42000, cum. loss 29.93, cum. ppl 4.27 cum. examples 63977\n",
            "begin validation ...\n",
            "validation: iter 42000, dev. ppl 7.687474\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "hit patience 2\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 7, iter 44000, cum. loss 29.63, cum. ppl 4.22 cum. examples 64000\n",
            "begin validation ...\n",
            "validation: iter 44000, dev. ppl 7.649515\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "hit patience 3\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 7, iter 46000, cum. loss 30.58, cum. ppl 4.42 cum. examples 64000\n",
            "begin validation ...\n",
            "validation: iter 46000, dev. ppl 7.547774\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "hit patience 4\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 8, iter 48000, cum. loss 29.88, cum. ppl 4.26 cum. examples 63977\n",
            "begin validation ...\n",
            "validation: iter 48000, dev. ppl 7.787341\n",
            "load previously best model and decay learning rate to 0.000500\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "hit patience 5\n",
            "hit #1 trial\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "restore parameters of the optimizers\n",
            "epoch 8, iter 50000, cum. loss 28.20, cum. ppl 3.95 cum. examples 64000\n",
            "begin validation ...\n",
            "validation: iter 50000, dev. ppl 7.231306\n",
            "save currently the best model to [/content/gdrive/MyDrive/Colab/model.bin]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "save model parameters to [/content/gdrive/MyDrive/Colab/model.bin]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 8, iter 52000, cum. loss 28.45, cum. ppl 3.95 cum. examples 64000\n",
            "begin validation ...\n",
            "validation: iter 52000, dev. ppl 7.203796\n",
            "save currently the best model to [/content/gdrive/MyDrive/Colab/model.bin]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "save model parameters to [/content/gdrive/MyDrive/Colab/model.bin]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 8, iter 54000, cum. loss 28.62, cum. ppl 4.03 cum. examples 64000\n",
            "begin validation ...\n",
            "validation: iter 54000, dev. ppl 7.217999\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "hit patience 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 9, iter 56000, cum. loss 26.25, cum. ppl 3.56 cum. examples 63977\n",
            "begin validation ...\n",
            "validation: iter 56000, dev. ppl 7.415108\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "hit patience 2\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 9, iter 58000, cum. loss 26.69, cum. ppl 3.65 cum. examples 64000\n",
            "begin validation ...\n",
            "validation: iter 58000, dev. ppl 7.423026\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "hit patience 3\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 9, iter 60000, cum. loss 26.98, cum. ppl 3.72 cum. examples 64000\n",
            "begin validation ...\n",
            "validation: iter 60000, dev. ppl 7.392202\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "hit patience 4\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 10, iter 62000, cum. loss 25.68, cum. ppl 3.49 cum. examples 63977\n",
            "begin validation ...\n",
            "validation: iter 62000, dev. ppl 7.685967\n",
            "load previously best model and decay learning rate to 0.000250\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "hit patience 5\n",
            "hit #2 trial\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "restore parameters of the optimizers\n",
            "epoch 10, iter 64000, cum. loss 26.10, cum. ppl 3.55 cum. examples 64000\n",
            "begin validation ...\n",
            "validation: iter 64000, dev. ppl 7.190749\n",
            "save currently the best model to [/content/gdrive/MyDrive/Colab/model.bin]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "save model parameters to [/content/gdrive/MyDrive/Colab/model.bin]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 10, iter 66000, cum. loss 26.16, cum. ppl 3.56 cum. examples 64000\n",
            "begin validation ...\n",
            "validation: iter 66000, dev. ppl 7.210836\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "hit patience 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 11, iter 68000, cum. loss 26.14, cum. ppl 3.55 cum. examples 63977\n",
            "begin validation ...\n",
            "validation: iter 68000, dev. ppl 7.298614\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "hit patience 2\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 11, iter 70000, cum. loss 24.81, cum. ppl 3.34 cum. examples 64000\n",
            "begin validation ...\n",
            "validation: iter 70000, dev. ppl 7.369444\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "hit patience 3\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 11, iter 72000, cum. loss 25.12, cum. ppl 3.39 cum. examples 64000\n",
            "begin validation ...\n",
            "validation: iter 72000, dev. ppl 7.387220\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "hit patience 4\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 11, iter 74000, cum. loss 25.49, cum. ppl 3.44 cum. examples 64000\n",
            "begin validation ...\n",
            "validation: iter 74000, dev. ppl 7.383981\n",
            "load previously best model and decay learning rate to 0.000125\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "hit patience 5\n",
            "hit #3 trial\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "restore parameters of the optimizers\n",
            "epoch 12, iter 76000, cum. loss 25.19, cum. ppl 3.41 cum. examples 63977\n",
            "begin validation ...\n",
            "validation: iter 76000, dev. ppl 7.209293\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "hit patience 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 12, iter 78000, cum. loss 25.39, cum. ppl 3.41 cum. examples 64000\n",
            "begin validation ...\n",
            "validation: iter 78000, dev. ppl 7.211154\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "hit patience 2\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 12, iter 80000, cum. loss 25.45, cum. ppl 3.44 cum. examples 64000\n",
            "begin validation ...\n",
            "validation: iter 80000, dev. ppl 7.200428\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "hit patience 3\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 13, iter 82000, cum. loss 24.96, cum. ppl 3.37 cum. examples 63977\n",
            "begin validation ...\n",
            "validation: iter 82000, dev. ppl 7.257249\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "hit patience 4\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 13, iter 84000, cum. loss 24.35, cum. ppl 3.26 cum. examples 64000\n",
            "begin validation ...\n",
            "validation: iter 84000, dev. ppl 7.320804\n",
            "load previously best model and decay learning rate to 0.000063\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "hit patience 5\n",
            "hit #4 trial\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "restore parameters of the optimizers\n",
            "epoch 13, iter 86000, cum. loss 25.29, cum. ppl 3.41 cum. examples 64000\n",
            "begin validation ...\n",
            "validation: iter 86000, dev. ppl 7.185739\n",
            "save currently the best model to [/content/gdrive/MyDrive/Colab/model.bin]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "save model parameters to [/content/gdrive/MyDrive/Colab/model.bin]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 13, iter 88000, cum. loss 25.16, cum. ppl 3.40 cum. examples 64000\n",
            "begin validation ...\n",
            "validation: iter 88000, dev. ppl 7.187486\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "hit patience 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 14, iter 90000, cum. loss 24.81, cum. ppl 3.34 cum. examples 63977\n",
            "begin validation ...\n",
            "validation: iter 90000, dev. ppl 7.196357\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "hit patience 2\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 14, iter 92000, cum. loss 24.75, cum. ppl 3.34 cum. examples 64000\n",
            "begin validation ...\n",
            "validation: iter 92000, dev. ppl 7.222061\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "hit patience 3\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 14, iter 94000, cum. loss 25.15, cum. ppl 3.37 cum. examples 64000\n",
            "begin validation ...\n",
            "validation: iter 94000, dev. ppl 7.203454\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "hit patience 4\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 15, iter 96000, cum. loss 24.46, cum. ppl 3.29 cum. examples 63977\n",
            "begin validation ...\n",
            "validation: iter 96000, dev. ppl 7.238782\n",
            "load previously best model and decay learning rate to 0.000031\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "hit patience 5\n",
            "hit #5 trial\n",
            "early stop!\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "restore parameters of the optimizers\n",
            "epoch 15, iter 98000, cum. loss 25.15, cum. ppl 3.38 cum. examples 64000\n",
            "begin validation ...\n",
            "validation: iter 98000, dev. ppl 7.193303\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "hit patience 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-9d7811becd81>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-13-48740bc1971b>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'decode'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-e44d5a27ca61>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0;31m# CALCULATE GRADIENT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0;31m# clip gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mgrad_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6hGJ2hb4FuQ"
      },
      "source": [
        "<div class='alert alert-block alert-info'>\n",
        "<b>ЗАПУСК</b>\n",
        "    <ul>\n",
        "        <li><i>Откуда берутся параметры, которые мы не указываем</i>\n",
        "        <li><b><i>Как запускать с определенного места?</i></b>\n",
        "    </ul>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1OcgMUtpNcS"
      },
      "source": [
        "test_args = {\n",
        " '--batch-size': '32',\n",
        " '--beam-size': '5',\n",
        " '--clip-grad': '5.0',\n",
        " '--cuda': True,\n",
        " '--dev-src': None,\n",
        " '--dev-tgt': None,\n",
        " '--dropout': '0.3',\n",
        " '--embed-size': '256',\n",
        " '--help': False,\n",
        " '--hidden-size': '256',\n",
        " '--input-feed': False,\n",
        " '--log-every': '10',\n",
        " '--lr': '0.001',\n",
        " '--lr-decay': '0.5',\n",
        " '--max-decoding-time-step': '70',\n",
        " '--max-epoch': '30',\n",
        " '--max-num-trial': '5',\n",
        " '--patience': '5',\n",
        " '--sample-size': '5',\n",
        " '--save-to': '/content/gdrive/MyDrive/Colab/model.bin',\n",
        " '--seed': '0',\n",
        " '--train-src': None,\n",
        " '--train-tgt': None,\n",
        " '--uniform-init': '0.1',\n",
        " '--valid-niter': '2000',\n",
        " '--vocab': None,\n",
        " 'MODEL_PATH': '/content/gdrive/MyDrive/Colab/model.bin',\n",
        " 'OUTPUT_FILE': '/content/gdrive/MyDrive/Colab/test_outputs.txt',\n",
        " 'TEST_SOURCE_FILE': '/content/gdrive/MyDrive/Colab/a4/en_es_data/test.es',\n",
        " 'TEST_TARGET_FILE': '/content/gdrive/MyDrive/Colab/a4/en_es_data/test.en',\n",
        " 'decode': True,\n",
        " 'train': False}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134,
          "referenced_widgets": [
            "b612f1fa45fe49a484445de5e7055d80",
            "dea1fc9dcb194fc9b814fc1199d304ce",
            "9d3d3af13b9c43e29f774e4ec580cdab",
            "20d00b598f9a4d90b2ea16b05cb237da",
            "34fb7f36719d46aea4cdfe4a8bbb2d07",
            "522303df1264487a81eed81c9a0c44da",
            "bdf946a77a834d8a810d3ce5e9b059d2",
            "e1148982764a4337b8bb914641915aa6"
          ]
        },
        "id": "j9pyOEZxctZP",
        "outputId": "8a87836f-8326-4cfe-dd63-7a0b6f41412f"
      },
      "source": [
        "main(test_args)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "load test source sentences from [/content/gdrive/MyDrive/Colab/a4/en_es_data/test.es]\n",
            "load test target sentences from [/content/gdrive/MyDrive/Colab/a4/en_es_data/test.en]\n",
            "load model from /content/gdrive/MyDrive/Colab/model.bin\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b612f1fa45fe49a484445de5e7055d80",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Decoding', max=8064.0, style=ProgressStyle(description_wi…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Corpus BLEU: 35.77854110593709\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SCLPZQhkdyHV"
      },
      "source": [
        "from tqdm.notebook import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}